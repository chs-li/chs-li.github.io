# 強化學習（RL）專案總覽

專注於遊戲 NPC 的路徑規劃與避障策略，在多個 Unity 場景上以單一 RL 模型實現移動與跳躍控制。透過隨機化環境、獎勵工程與策略穩定化技術，使模型能在不同地形與障礙配置下具備良好泛化能力。最終整合至 Unity 進行實機驗證。

## 主要亮點
- 同時支援 **連續與離散** 動作空間，行走與跳躍可共訓。
- 高度 **隨機化環境**（起點、終點、障礙類型、角色大小等），有效提升泛化能力。
- 以路徑平滑度與目標點導向設計獎勵，減少不自然路徑。
- 主力採用 **PPO + LSTM**，搭配 curriculums、LR 調整與 reward shaping 提升穩定性。
- 與 Unity 整合，透過服務介面進行動作推理與多版本場景測試。

## 內容概覽
### 核心避障專案（Character Movement）
- **環境設定**：多種地形與障礙布局，包含靜態與動態障礙；移動與跳躍皆可學習。
- **觀測設計**：結合角色狀態、目標位置與周遭障礙資訊；視野感測可依場景調整。
- **獎勵工程**：包含路徑進展、平滑度、跳躍合理性與存活獎勵，避免策略震盪。
- **演算法與調參**：以 PPO 為主，並測試 LSTM、SAC、行為克隆暖啟與課程式訓練；透過 LR、clip、獎勵比重等調整避免策略崩塌。
- **訓練迭代方向**：由簡單場景逐步增加障礙數量、隨機性與行為複雜度，成功率與平滑度隨訓練演進穩定提升。

### Unity 整合
- 透過輕量 API 或本地推理服務在 Unity 場景中回傳動作（移動/跳躍）。
- 支援多場景測試（靜態/動態障礙），驗證視野設定、延遲與物理參數對策略的影響。

### 其他實驗
- 遊戲互動策略：嘗試 DQN、SAC、行為克隆與 GAIL 等方法。
- 工具與流程：包含 ML-Agents 使用經驗、環境封裝與部署腳本。

## 使用與部署建議
- **快速上線避障**：建議使用 PPO+LSTM 與平滑度導向獎勵，並依場景調整視野與障礙數量。
- **高隨機性場景**：增加觀測涵蓋度並採用分階段課程訓練。
- **策略不穩定時**：降低學習率、放緩 clip 或重新調整平滑度相關獎勵；必要時加入行為克隆暖啟。
- **實機驗證**：建議先在模擬環境穩定後再掛載 Unity，確保感測設定與服務延遲一致。

